{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Using other python AI LLM packages - Transformers & pyTorch examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Copyright 2024-today Dr. George Papagiannakis,  papagian@csd.uoc.gr*\n",
    "*All Rights Reserved*\n",
    "### *University of Crete & Foundation for Research & Technology - Hellas (FORTH)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example `Transformers` script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install `pip install 'transformers[torch]` to run this script. You also need to install pyTorch as backend for the transformers package: `pip install 'transformers[torch]' `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "\n",
    "# Bug: ValueError: Tokenizer class LlamaTokenizer does not exist or is not currently imported.\n",
    "# Solution: pip3 install sentencepiece\n",
    "# Info: https://github.com/huggingface/transformers/issues/22222\n",
    "\n",
    "# Bug: ImportError: cannot import name 'LlamaTokenizer' from 'transformers'\n",
    "# Solution: pip3 install git+https://github.com/huggingface/transformers\n",
    "# Info: https://stackoverflow.com/questions/75907910/importerror-cannot-import-name-llamatokenizer-from-transformers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"Input your prompt: \")\n",
    "\n",
    "    # https://stackoverflow.com/questions/74748116/huggingface-automodelforcasuallm-decoder-only-architecture-warning-even-after\n",
    "    input_ids = tokenizer.encode(tokenizer.eos_token + prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    print('generating response...')\n",
    "    output = model.generate(input_ids, max_length=20, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Response: \", decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example `pyTorch` script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install `pip install torch` to run this script or for Apple Silicon M1: `conda install pytorch torchvision torchaudio -c pytorch-nightly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to verify pyTorch is working\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example to verify pyTorch is working\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example `Tensorflow` script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to instsall `pip install tensorflow` to run this script. Also for Apple Silicon M1: `pip install tensorflow-macos` and `pip install tensorflow_datasets` as well as `conda install -c apple tensorflow-deps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to verify TensorFlow is working\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elementsProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
